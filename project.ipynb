{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some final variables\n",
    "image_size = 299\n",
    "batch_size = 128\n",
    "is_debug = False\n",
    "train_folder = \"sub_train/\" if is_debug else \"train/\"\n",
    "cat_count = 50 if is_debug else 12500\n",
    "dog_count = 50 if is_debug else 12500\n",
    "best_model_path = 'best_point_' + str(image_size) + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_files():\n",
    "    train_files = []\n",
    "    for i in range(cat_count):\n",
    "        train_files.append(train_folder + \"cat.\" + str(i) + \".jpg\")\n",
    "        \n",
    "    for i in range(dog_count):\n",
    "        train_files.append(train_folder + \"dog.\" + str(i) + \".jpg\")\n",
    "        \n",
    "    return train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/cat.835.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/dog.2317.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "train_files = get_train_files()\n",
    "train_files = sorted(train_files, key=lambda s: (len(s), s))\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for i in range(len(train_files)):\n",
    "    f = train_files[i]\n",
    "    im = Image.open(f)\n",
    "    width, height = im.size\n",
    "    if (width > 600 or height > 600):\n",
    "        img = mpimg.imread(f)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(f)\n",
    "    xs.append(width)\n",
    "    ys.append(height)\n",
    "x_array = np.array(xs)\n",
    "y_array = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('width and height')\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n",
    "plt.scatter(x_array, y_array)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "inceptionV3 = InceptionV3(weights='imagenet')  # 引入InceptionV3模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "\n",
    "def readImage(path):\n",
    "    img = image.load_img(path, target_size=(image_size, image_size))\n",
    "    x = image.img_to_array(img)\n",
    "    \n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "#     x = x.astype('float32')\n",
    "#     x /= 255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(path):\n",
    "    name = path[path.rfind('/') + 1:path.index('.')]\n",
    "#     print (name)\n",
    "    return 1 if name == 'dog' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = [\n",
    " 'n02085620','n02085782','n02085936','n02086079'\n",
    ",'n02086240','n02086646','n02086910','n02087046'\n",
    ",'n02087394','n02088094','n02088238','n02088364'\n",
    ",'n02088466','n02088632','n02089078','n02089867'\n",
    ",'n02089973','n02090379','n02090622','n02090721'\n",
    ",'n02091032','n02091134','n02091244','n02091467'\n",
    ",'n02091635','n02091831','n02092002','n02092339'\n",
    ",'n02093256','n02093428','n02093647','n02093754'\n",
    ",'n02093859','n02093991','n02094114','n02094258'\n",
    ",'n02094433','n02095314','n02095570','n02095889'\n",
    ",'n02096051','n02096177','n02096294','n02096437'\n",
    ",'n02096585','n02097047','n02097130','n02097209'\n",
    ",'n02097298','n02097474','n02097658','n02098105'\n",
    ",'n02098286','n02098413','n02099267','n02099429'\n",
    ",'n02099601','n02099712','n02099849','n02100236'\n",
    ",'n02100583','n02100735','n02100877','n02101006'\n",
    ",'n02101388','n02101556','n02102040','n02102177'\n",
    ",'n02102318','n02102480','n02102973','n02104029'\n",
    ",'n02104365','n02105056','n02105162','n02105251'\n",
    ",'n02105412','n02105505','n02105641','n02105855'\n",
    ",'n02106030','n02106166','n02106382','n02106550'\n",
    ",'n02106662','n02107142','n02107312','n02107574'\n",
    ",'n02107683','n02107908','n02108000','n02108089'\n",
    ",'n02108422','n02108551','n02108915','n02109047'\n",
    ",'n02109525','n02109961','n02110063','n02110185'\n",
    ",'n02110341','n02110627','n02110806','n02110958'\n",
    ",'n02111129','n02111277','n02111500','n02111889'\n",
    ",'n02112018','n02112137','n02112350','n02112706'\n",
    ",'n02113023','n02113186','n02113624','n02113712'\n",
    ",'n02113799','n02113978']\n",
    "\n",
    "cats=[\n",
    "'n02123045','n02123159','n02123394','n02123597'\n",
    ",'n02124075','n02125311','n02127052']\n",
    "\n",
    "def isCat(preds): # 判断是否为猫\n",
    "    for item in preds:\n",
    "        if item[0] in cats:\n",
    "            return True\n",
    "    return False\n",
    "def isDog(preds): # 判断是否为狗\n",
    "    for item in preds:\n",
    "        if item[0] in dogs:\n",
    "            return True\n",
    "    return False\n",
    "def isValid(img_path): # 判断是否为合理的图片\n",
    "    label = getLabel(img_path)\n",
    "    preds = getPreds(img_path)\n",
    "    return {\n",
    "        0:isCat(preds),\n",
    "        1:isDog(preds)\n",
    "    }[label]\n",
    "\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "def getPreds(img_path): # 获取模型对某一张图片的预测top-50\n",
    "    img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = inceptionV3.predict(x)\n",
    "    result = decode_predictions(preds, top=50)[0]\n",
    "#     print('Predicted:', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 need to preprocess\n",
      "25000 files validpg\n",
      "\n",
      "78 invalid files as below:\n",
      "['train/cat.252.jpg', 'train/cat.335.jpg', 'train/cat.1485.jpg', 'train/cat.2337.jpg', 'train/cat.2433.jpg', 'train/cat.2457.jpg', 'train/cat.2509.jpg', 'train/cat.2621.jpg', 'train/cat.2893.jpg', 'train/cat.2939.jpg', 'train/cat.3216.jpg', 'train/cat.3399.jpg', 'train/cat.3658.jpg', 'train/cat.3731.jpg', 'train/cat.3738.jpg', 'train/cat.3845.jpg', 'train/cat.3868.jpg', 'train/cat.4338.jpg', 'train/cat.4688.jpg', 'train/cat.4833.jpg', 'train/cat.4852.jpg', 'train/cat.4965.jpg', 'train/cat.5324.jpg', 'train/cat.5351.jpg', 'train/cat.5355.jpg', 'train/cat.5418.jpg', 'train/cat.5527.jpg', 'train/cat.5820.jpg', 'train/cat.6345.jpg', 'train/cat.6402.jpg', 'train/cat.6429.jpg', 'train/cat.6442.jpg', 'train/cat.6699.jpg', 'train/cat.6915.jpg', 'train/cat.7194.jpg', 'train/cat.7291.jpg', 'train/cat.7377.jpg', 'train/cat.7411.jpg', 'train/cat.7487.jpg', 'train/cat.7564.jpg', 'train/cat.7671.jpg', 'train/cat.7920.jpg', 'train/cat.7968.jpg', 'train/cat.8456.jpg', 'train/cat.8470.jpg', 'train/cat.8854.jpg', 'train/cat.8921.jpg', 'train/cat.9171.jpg', 'train/cat.9444.jpg', 'train/cat.9456.jpg', 'train/cat.9983.jpg', 'train/cat.9987.jpg', 'train/dog.1773.jpg', 'train/dog.1895.jpg', 'train/dog.2422.jpg', 'train/dog.2614.jpg', 'train/dog.4367.jpg', 'train/dog.5604.jpg', 'train/dog.6475.jpg', 'train/dog.8736.jpg', 'train/cat.10029.jpg', 'train/cat.10700.jpg', 'train/cat.10712.jpg', 'train/cat.10946.jpg', 'train/cat.11184.jpg', 'train/cat.11222.jpg', 'train/cat.11231.jpg', 'train/cat.11565.jpg', 'train/cat.11607.jpg', 'train/cat.11777.jpg', 'train/cat.12227.jpg', 'train/cat.12272.jpg', 'train/cat.12378.jpg', 'train/dog.10161.jpg', 'train/dog.10190.jpg', 'train/dog.10237.jpg', 'train/dog.10801.jpg', 'train/dog.12376.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess pics\n",
    "# import helper\n",
    "\n",
    "valid_files = []\n",
    "invalid_files = []\n",
    "train_files = get_train_files()\n",
    "train_files = sorted(train_files, key=lambda s: (len(s), s))\n",
    "\n",
    "count = len(train_files)\n",
    "\n",
    "print(str(count) + \" need to preprocess\")\n",
    "for i in range(count):\n",
    "    file = train_files[i]\n",
    "    \n",
    "    print (file, end='\\r')\n",
    "    if isValid(file):\n",
    "        valid_files.append(file)\n",
    "    else:\n",
    "#         os.remove(file)\n",
    "        invalid_files.append(file)\n",
    "\n",
    "print (str(count) + \" files valid\\n\")\n",
    "print (str(len(invalid_files)) + \" invalid files as below:\\n\", end='\\r')\n",
    "print (invalid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFeatureAndLabel(valid_files):\n",
    "    file_count = len(valid_files)\n",
    "    print(\"readFeatureAndLabel file_count=\" + str(file_count))\n",
    "    train_feature = []\n",
    "    train_label = []\n",
    "    for i in range(file_count):\n",
    "        f = train_files[i]\n",
    "#         print (f)\n",
    "        train_feature.append(readImage(f))\n",
    "        train_label.append(getLabel(f))\n",
    "    \n",
    "    return train_feature, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readFeatureAndLabel file_count=24922\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-eccb1072ce85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtotal_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "total_feature, total_label = readFeatureAndLabel(valid_files)\n",
    "total_feature = np.array(total_feature)\n",
    "\n",
    "total_label = np.array(total_label)\n",
    "\n",
    "train_feature, validation_feature, train_label, validation_label = train_test_split(total_feature, total_label, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan C\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "# 创建预训练模型\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# 添加平均池化层\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# 由于我们只需要狗的概率，所以设置参数classes为1，输出函数为sigmoid\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 创建模型\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# optimizer='rmsprop'\n",
    "# , beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "# history = model.fit_generator(train_gen, epochs=10, validation_data=val_gen)\n",
    "history = model.fit(train_feature, train_label, batch_size=batch_size, epochs=20, validation_data=(validation_feature, validation_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "# loss='sparse_categorical_crossentropy'\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout\n",
    "# Adam(lr=0.001)\n",
    "model.add(Dropout(0.2))\n",
    "opt2 = SGD(lr=0.0008, momentum=0.99, decay=0.00002, nesterov=True)\n",
    "model.compile(optimizer=opt2, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "callback = ModelCheckpoint(best_model_path, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "history2 = model.fit(train_feature, train_label, batch_size=batch_size, epochs=10, callbacks=[callback], validation_data=(validation_feature, validation_label), class_weight='auto')\n",
    "# history2 = model.fit_generator(train_gen, epochs=10, validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history2.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history2.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history2.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history2.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def score_fun(label_pre):\n",
    "    total_score = 0\n",
    "    size = validation_label.size\n",
    "    for i in range(size):\n",
    "        y = validation_label[i]\n",
    "        yp = label_pre[i]\n",
    "#         print (str(y) + \" \" + str(yp))\n",
    "        try:\n",
    "            total_score = y * math.log(yp) + (1 - y)*math.log(1-yp)\n",
    "        except:\n",
    "            print (str(i) + \": \" + str(y) + \" - \" + str(yp))\n",
    "    return -total_score/size\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_pre = load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_pre = model_pre.predict(validation_feature)\n",
    "val_label_pre = np.clip(val_label_pre, 0.01, 0.99)\n",
    "score = score_fun(val_label_pre)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test data\n",
    "\n",
    "import helper\n",
    "\n",
    "test_files = helper.get_test_files()\n",
    "test_files = sorted(test_files, key=lambda s: (len(s), s))\n",
    "\n",
    "test_file_size = len(test_files)\n",
    "\n",
    "test_feature = []\n",
    "for i in range(test_file_size):\n",
    "    test_feature.append(readImage(test_files[i]))\n",
    "test_feature = np.asarray(test_feature)\n",
    "print(type(test_feature))\n",
    "test_label = model_pre.predict(test_feature, batch_size=batch_size)\n",
    "print (len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out predict data to csv\n",
    "\n",
    "test_label_output = []\n",
    "test_label_clip = np.clip(test_label, 0.01, 0.99)\n",
    "for i in range(len(test_label_clip)):\n",
    "    test_label_output.append([i + 1, test_label_clip[i]])\n",
    "    \n",
    "print (len(test_label_output))\n",
    "test_label_output = np.array(test_label_output)\n",
    "np.savetxt(\"submission.csv\", test_label_output, fmt='%d,%f', delimiter=',', header=\"id,label\", comments=\"\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
