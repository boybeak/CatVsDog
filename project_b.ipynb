{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some final variables\n",
    "imageSize = 224\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def readImage(path):\n",
    "    img = image.load_img(path, target_size=(imageSize, imageSize))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x.astype('float32')\n",
    "    x /= 255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(path):\n",
    "    name = path[path.rfind('/') + 1:path.index('.')]\n",
    "#     print (name)\n",
    "    return 1 if name == 'dog' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "def getPreds(img_path):\n",
    "    img = image.load_img(img_path, target_size=(imageSize, imageSize))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model.predict(x)\n",
    "    result = decode_predictions(preds, top=50)[0]\n",
    "#     print('Predicted:', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = [\n",
    " 'n02085620','n02085782','n02085936','n02086079'\n",
    ",'n02086240','n02086646','n02086910','n02087046'\n",
    ",'n02087394','n02088094','n02088238','n02088364'\n",
    ",'n02088466','n02088632','n02089078','n02089867'\n",
    ",'n02089973','n02090379','n02090622','n02090721'\n",
    ",'n02091032','n02091134','n02091244','n02091467'\n",
    ",'n02091635','n02091831','n02092002','n02092339'\n",
    ",'n02093256','n02093428','n02093647','n02093754'\n",
    ",'n02093859','n02093991','n02094114','n02094258'\n",
    ",'n02094433','n02095314','n02095570','n02095889'\n",
    ",'n02096051','n02096177','n02096294','n02096437'\n",
    ",'n02096585','n02097047','n02097130','n02097209'\n",
    ",'n02097298','n02097474','n02097658','n02098105'\n",
    ",'n02098286','n02098413','n02099267','n02099429'\n",
    ",'n02099601','n02099712','n02099849','n02100236'\n",
    ",'n02100583','n02100735','n02100877','n02101006'\n",
    ",'n02101388','n02101556','n02102040','n02102177'\n",
    ",'n02102318','n02102480','n02102973','n02104029'\n",
    ",'n02104365','n02105056','n02105162','n02105251'\n",
    ",'n02105412','n02105505','n02105641','n02105855'\n",
    ",'n02106030','n02106166','n02106382','n02106550'\n",
    ",'n02106662','n02107142','n02107312','n02107574'\n",
    ",'n02107683','n02107908','n02108000','n02108089'\n",
    ",'n02108422','n02108551','n02108915','n02109047'\n",
    ",'n02109525','n02109961','n02110063','n02110185'\n",
    ",'n02110341','n02110627','n02110806','n02110958'\n",
    ",'n02111129','n02111277','n02111500','n02111889'\n",
    ",'n02112018','n02112137','n02112350','n02112706'\n",
    ",'n02113023','n02113186','n02113624','n02113712'\n",
    ",'n02113799','n02113978']\n",
    "\n",
    "cats=[\n",
    "'n02123045','n02123159','n02123394','n02123597'\n",
    ",'n02124075','n02125311','n02127052']\n",
    "\n",
    "def isCat(preds):\n",
    "    for item in preds:\n",
    "        if item[0] in cats:\n",
    "            return True\n",
    "    return False\n",
    "def isDog(preds):\n",
    "    for item in preds:\n",
    "        if item[0] in dogs:\n",
    "            return True\n",
    "    return False\n",
    "def isValid(img_path):\n",
    "    label = getLabel(img_path)\n",
    "    preds = getPreds(img_path)\n",
    "    return {\n",
    "        0:isCat(preds),\n",
    "        1:isDog(preds)\n",
    "    }[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 need to preprocess\n",
      "24836 files validCatVsDog/train/dog.12499.jpg\n",
      "\n",
      "164 invalid files as below:\n",
      "\r"
     ]
    }
   ],
   "source": [
    "# Preprocess pics\n",
    "import helper\n",
    "\n",
    "\n",
    "train_dir = 'train/'\n",
    "valid_files = []\n",
    "invalid_files = []\n",
    "train_files = helper.get_train_files()\n",
    "train_files = sorted(train_files, key=lambda s: (len(s), s))\n",
    "\n",
    "print(str(len(train_files)) + \" need to preprocess\")\n",
    "\n",
    "for i in range(len(train_files)):\n",
    "    file = train_files[i]\n",
    "    print (file, end='\\r')\n",
    "    if isValid(file):\n",
    "        valid_files.append(file)\n",
    "    else:\n",
    "        invalid_files.append(file)\n",
    "\n",
    "print (str(len(valid_files)) + \" files valid\\n\")\n",
    "print (str(len(invalid_files)) + \" invalid files as below:\\n\", end='\\r')\n",
    "# print (invalid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFeatureAndLabel(valid_files):\n",
    "    file_count = len(valid_files)\n",
    "    print(\"readFeatureAndLabel file_count=\" + str(file_count))\n",
    "    train_feature = []\n",
    "    train_label = []\n",
    "    for i in range(file_count):\n",
    "        f = train_files[i]\n",
    "#         print (f)\n",
    "        train_feature.append(readImage(f))\n",
    "        train_label.append(getLabel(f))\n",
    "    \n",
    "    return train_feature, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readFeatureAndLabel file_count=24836\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "total_feature, total_label = readFeatureAndLabel(valid_files)\n",
    "total_feature = np.array(total_feature)\n",
    "\n",
    "total_label = np.array(total_label)\n",
    "\n",
    "train_feature, validation_feature, train_label, validation_label = train_test_split(total_feature, total_label, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22352 samples, validate on 2484 samples\n",
      "Epoch 1/30\n",
      "21632/22352 [============================>.] - ETA: 1s - loss: 0.3097 - acc: 0.8937"
     ]
    }
   ],
   "source": [
    "# Plan C\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# optimizer='rmsprop'\n",
    "# , beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_feature, train_label, batch_size=batch_size, epochs=30, validation_data=(validation_feature, validation_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22352 samples, validate on 2484 samples\n",
      "Epoch 1/20\n",
      "22352/22352 [==============================] - 49s 2ms/step - loss: 0.0872 - acc: 0.9647 - val_loss: 0.0362 - val_acc: 0.9891\n",
      "Epoch 2/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0354 - val_acc: 0.9907\n",
      "Epoch 3/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0376 - val_acc: 0.9903\n",
      "Epoch 4/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.0356 - val_acc: 0.9903\n",
      "Epoch 5/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.0407 - val_acc: 0.9911\n",
      "Epoch 6/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.0373 - val_acc: 0.9907\n",
      "Epoch 7/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0377 - val_acc: 0.9911\n",
      "Epoch 8/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0393 - val_acc: 0.9907\n",
      "Epoch 9/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 8.6994e-04 - acc: 0.9999 - val_loss: 0.0362 - val_acc: 0.9911\n",
      "Epoch 10/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 7.8487e-04 - acc: 0.9999 - val_loss: 0.0365 - val_acc: 0.9907\n",
      "Epoch 11/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 5.6461e-04 - acc: 0.9999 - val_loss: 0.0406 - val_acc: 0.9903\n",
      "Epoch 12/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 6.1017e-04 - acc: 1.0000 - val_loss: 0.0452 - val_acc: 0.9899\n",
      "Epoch 13/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 4.2415e-04 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 0.9907\n",
      "Epoch 14/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 3.4588e-04 - acc: 1.0000 - val_loss: 0.0375 - val_acc: 0.9911\n",
      "Epoch 15/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 3.8374e-04 - acc: 1.0000 - val_loss: 0.0375 - val_acc: 0.9911\n",
      "Epoch 16/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 4.3577e-04 - acc: 1.0000 - val_loss: 0.0378 - val_acc: 0.9915\n",
      "Epoch 17/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 3.8268e-04 - acc: 1.0000 - val_loss: 0.0387 - val_acc: 0.9911\n",
      "Epoch 18/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 3.0089e-04 - acc: 1.0000 - val_loss: 0.0373 - val_acc: 0.9911\n",
      "Epoch 19/20\n",
      "22352/22352 [==============================] - 40s 2ms/step - loss: 2.5671e-04 - acc: 1.0000 - val_loss: 0.0375 - val_acc: 0.9915\n",
      "Epoch 20/20\n",
      "22352/22352 [==============================] - 41s 2ms/step - loss: 4.6446e-04 - acc: 0.9999 - val_loss: 0.0351 - val_acc: 0.9919\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "# loss='sparse_categorical_crossentropy'\n",
    "from keras.optimizers import SGD\n",
    "# Adam(lr=0.001)\n",
    "opt2 = SGD(lr=0.0006, momentum=0.99, decay=0.00002, nesterov=True)\n",
    "model.compile(optimizer=opt2, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history2 = model.fit(train_feature, train_label, batch_size=batch_size, epochs=20, validation_data=(validation_feature, validation_label), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history2.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history2.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history2.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history2.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def score_fun(label_pre):\n",
    "    total_score = 0\n",
    "    size = validation_label.size\n",
    "    for i in range(size):\n",
    "        y = validation_label[i]\n",
    "        yp = label_pre[i]\n",
    "#         print (str(y) + \" \" + str(yp))\n",
    "        try:\n",
    "            total_score = y * math.log(yp) + (1 - y)*math.log(1-yp)\n",
    "        except:\n",
    "            print (str(i) + \": \" + str(y) + \" - \" + str(yp))\n",
    "    return -total_score/size\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_pre = model.predict(validation_feature)\n",
    "val_label_pre = np.clip(val_label_pre, 0.01, 0.99)\n",
    "score = score_fun(val_label_pre)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test data\n",
    "\n",
    "import helper\n",
    "\n",
    "test_files = helper.get_test_files()\n",
    "test_files = sorted(test_files, key=lambda s: (len(s), s))\n",
    "\n",
    "test_file_size = len(test_files)\n",
    "\n",
    "test_feature = []\n",
    "for i in range(test_file_size):\n",
    "    test_feature.append(readImage(test_files[i]))\n",
    "test_feature = np.asarray(test_feature)\n",
    "print(type(test_feature))\n",
    "test_label = model.predict(test_feature, batch_size=batch_size)\n",
    "print (len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out predict data to csv\n",
    "\n",
    "test_label_output = []\n",
    "test_label_clip = np.clip(test_label, 0.01, 0.99)\n",
    "for i in range(len(test_label_clip)):\n",
    "    test_label_output.append([i + 1, test_label_clip[i]])\n",
    "    \n",
    "print (len(test_label_output))\n",
    "test_label_output = np.array(test_label_output)\n",
    "np.savetxt(\"submission.csv\", test_label_output, fmt='%d,%f', delimiter=',', header=\"id,label\", comments=\"\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
